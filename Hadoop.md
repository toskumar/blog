# Hadoop
Hadoop is a framework that allows for the distributed storage, processing of large data sets across clusters of computers using simple programming models.

## Modules
* Hadoop Common: The common utilities that support the other Hadoop modules.
* Hadoop Distributed File System (HDFS): A distributed file system that provides high-throughput access to application data.
* Hadoop YARN: A framework for job scheduling and cluster resource management.
* Hadoop MapReduce: A YARN-based system for parallel processing of large data sets.
* Hadoop Ozone: A scalable, redundant, and distributed object store which is built on a highly available, replicated block storage layer called Hadoop Distributed Data Store (HDDS).

## Projects
* Ambari: A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters.
* Avro: A data serialization system.
* Cassandra: A scalable multi-master database with no single points of failure.
* Chukwa: A data collection system for managing large distributed systems.
* HBase: A scalable, distributed database that supports structured data storage for large tables.
* Hive: A data warehouse infrastructure that provides data summarization and ad hoc querying.
* Mahout: A Scalable machine learning and data mining library.
* Pig: A high-level data-flow language and execution framework for parallel computation.
* Spark: A fast and general compute engine for Hadoop data
* Submarine: A unified AI platform which allows engineers and data scientists to run Machine Learning and Deep Learning workload in distributed cluster.
* Tez:A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases.
* ZooKeeper: A high-performance coordination service for distributed applications.
